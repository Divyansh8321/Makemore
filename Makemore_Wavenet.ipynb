{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61a9ad37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a582009c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open( 'names.txt', 'r' ).read().splitlines()\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "\n",
    "vocab_size = len(stoi)                \n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ef7c4da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10915e470>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random \n",
    "random.seed(47)\n",
    "random.shuffle(words)\n",
    "\n",
    "\n",
    "torch.manual_seed(47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "567dd6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the dataset \n",
    "\n",
    "block_size = 8 # Context length \n",
    "\n",
    "def build_dataset(words):\n",
    "    X , Y = [], []\n",
    "    \n",
    "    for w in words :\n",
    "        context = [0]*block_size\n",
    "        for ch in w + '.' :\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "            \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    \n",
    "    return X , Y \n",
    "n1 = int(len(words) * 0.8)\n",
    "n2 = int(len(words) * 0.9)\n",
    "\n",
    "X_train , Y_train = build_dataset(words[:n1])\n",
    "X_cv , Y_cv = build_dataset(words[n1:n2])\n",
    "X_test , Y_test = build_dataset(words[n2:])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d4ea7544",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........ --> {tensor(11)}\n",
      ".......k --> {tensor(5)}\n",
      "......ke --> {tensor(12)}\n",
      ".....kel --> {tensor(19)}\n",
      "....kels --> {tensor(25)}\n",
      "...kelsy --> {tensor(0)}\n",
      "........ --> {tensor(2)}\n",
      ".......b --> {tensor(18)}\n",
      "......br --> {tensor(25)}\n",
      ".....bry --> {tensor(3)}\n",
      "....bryc --> {tensor(5)}\n",
      "...bryce --> {tensor(14)}\n",
      "..brycen --> {tensor(14)}\n",
      ".brycenn --> {tensor(0)}\n",
      "........ --> {tensor(13)}\n",
      ".......m --> {tensor(1)}\n",
      "......ma --> {tensor(18)}\n",
      ".....mar --> {tensor(20)}\n",
      "....mart --> {tensor(1)}\n",
      "...marta --> {tensor(22)}\n"
     ]
    }
   ],
   "source": [
    "for x , y in zip(X_train[:20] , Y_train[:20]):\n",
    "    print (''.join(itos[ix.item()] for ix in x) , '-->' , {y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2155b2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again creating a smaller version of pytorch just for the layers\n",
    "\n",
    "class Linear:\n",
    "    \n",
    "    def __init__(self , fan_in , fan_out , bias = True):\n",
    "        self.weights = torch.randn(fan_in , fan_out)/(fan_in**0.5) # kaiming init\n",
    "        self.bias = torch.randn(fan_out) if bias else None\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weights\n",
    "        if self.bias is not None :\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.weights] + ([] if self.bias is None else [self.bias])\n",
    "    \n",
    "class BatchNorm1D:\n",
    "    \n",
    "# Have to wait for the running mean and running variance to converge ; Can say that the layer has a state and state\n",
    "# in a layer is harmful\n",
    "    \n",
    "    def __init__(self, dim , eps = 1e-5 , momentum = 0.9):\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "        self.training = True\n",
    "        # parameters (trained with backpropagation)\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "        # parameters to be kept track of \n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_var = torch.ones(dim)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        \n",
    "        if self.training :    \n",
    "            if x.ndim == 2:\n",
    "                dim = 0\n",
    "            elif x.ndim == 3:\n",
    "                dim = (0,1)\n",
    "        # Allows more numbers to be used to calculate mean and variance so they are better \n",
    "        \n",
    "            xmean = x.mean(dim, keepdim=True) # batch mean\n",
    "            xvar = x.var(dim, keepdim=True) # batch variance\n",
    "        else :\n",
    "            xmean = self.running_mean\n",
    "            xvar = self.running_var\n",
    "        \n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "        self.out = self.gamma * xhat + self.beta \n",
    "        \n",
    "        # Exponential moving average for mean and variance    \n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "        return self.out\n",
    "        \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "    \n",
    "class Tanh:\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return [ ]    \n",
    "    \n",
    "    \n",
    "# The following classes have been developed to make it easier for us to define the network and make it cleaner\n",
    "# This is sort of generating LEGO blocks which can then be placed one over the other \n",
    "\n",
    "class Embeddings:\n",
    "    \n",
    "    def __init__(self , num_class , dims):\n",
    "        self.weights = torch.randn(num_class, dims)\n",
    "        \n",
    "    def __call__(self, IX):\n",
    "        self.out = self.weights[IX]\n",
    "        return self.out\n",
    "        \n",
    "    def parameters(self):\n",
    "        return [self.weights]\n",
    "    \n",
    "    \n",
    "class FlattenCons:\n",
    "    \n",
    "    def __init__(self, dims):\n",
    "        self.dims = dims\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        B, T, C = x.shape\n",
    "        x = x.view(B, T // self.dims, C * self.dims)\n",
    "\n",
    "        if x.shape[1] == 1:\n",
    "            x = x.squeeze(1)\n",
    "\n",
    "        self.out = x\n",
    "        return self.out\n",
    " \n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "    \n",
    "class Sequential:\n",
    "    \n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "  \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        self.out = x\n",
    "        return self.out\n",
    "  \n",
    "    def parameters(self):\n",
    "        # get parameters of all layers and stretch them out into one list\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "    \n",
    "    \n",
    "# The matmult operator in torch is pretty strong and it doesn't necessary multiple just 2 dimensional matrices;\n",
    "# It is well capable of doing 3 dim x 2 dim and the last dim and the first dim of the former and later will\n",
    "# dissappear as they do in matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6fedc14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176875\n"
     ]
    }
   ],
   "source": [
    "# Simplified and cleaner architecture with Wavenet Pattern wherein through each iteration of a hidden layer \n",
    "# 2 elements are combined into 1 and make a new batch dimension. So rather than all flattening all 8 of the \n",
    "# input character representations , we will flatten them in steps of 2 ; and these bigrams or fourgrams or .. will\n",
    "# be multiplied by the weights of the incoming layers. \n",
    "\n",
    "# For example :  [4 (1st batch dim), 8 (2nd batch dim), 10] first goes to [4,4,20] and this goes to \n",
    "# [4,4,200 (n_hidden)] then this goes to [4,2,400] and then this goes to [4,2,200] and then this goes to \n",
    "# [4,400] and then all of the characters have now been flattened. \n",
    "\n",
    "\n",
    "# For clarity think that 4 sets of 8,10 are transformed into 4 sets of 4,20 and then 4 sets of 4, 200 are converted\n",
    "# to 4 sets of 2 , 400\n",
    "\n",
    "n_embd = 24 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "model = Sequential([\n",
    "  Embeddings(vocab_size, n_embd),\n",
    "  FlattenCons(2), Linear(n_embd * 2, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),\n",
    "  FlattenCons(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),\n",
    "  FlattenCons(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),\n",
    "  Linear(n_hidden, vocab_size),\n",
    "])\n",
    "\n",
    "\n",
    "# parameter init\n",
    "with torch.no_grad():\n",
    "    model.layers[-1].weights *= 0.1 # last layer make less confident\n",
    "\n",
    "parameters = model.parameters()\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9b221c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 2.0154\n",
      "  10000/ 200000: 2.1073\n",
      "  20000/ 200000: 1.9573\n",
      "  30000/ 200000: 1.6970\n",
      "  40000/ 200000: 2.1753\n",
      "  50000/ 200000: 1.9116\n",
      "  60000/ 200000: 1.6498\n",
      "  70000/ 200000: 1.8736\n",
      "  80000/ 200000: 1.1132\n",
      "  90000/ 200000: 1.7419\n",
      " 100000/ 200000: 1.5274\n",
      " 110000/ 200000: 1.9960\n",
      " 120000/ 200000: 1.3744\n"
     ]
    }
   ],
   "source": [
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "  \n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, X_train.shape[0], (batch_size,))\n",
    "    Xb, Yb = X_train[ix], Y_train[ix] # batch X,Y\n",
    "  \n",
    "    # forward pass\n",
    "    logits = model(Xb)\n",
    "    loss = F.cross_entropy(logits, Yb) # loss function\n",
    "  \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "  \n",
    "    # update: simple SGD\n",
    "    lr = 0.1 if i < 150000 else 0.01 # step learning rate decay\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95410f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print (layer.__class__.__name__, ':' , tuple(layer.out.shape)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed3e395",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.tensor(lossi).view(-1,1000).mean(1)) # You get about 200 data points and these are average values\n",
    "# of the sets of 1000 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351020a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put layers into eval mode (needed for batchnorm especially)\n",
    "for layer in model.layers:\n",
    "    layer.training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606ddc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the loss\n",
    "@torch.no_grad() # this decorator disables gradient tracking inside pytorch\n",
    "def split_loss(split):\n",
    "    x,y = {\n",
    "    'train': (X_train, Y_train),\n",
    "    'val': (X_cv, Y_cv),\n",
    "    'test': (X_test, Y_test),\n",
    "  }[split]\n",
    "    logits = model(x)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48f30f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from the model\n",
    "\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      # forward pass the neural net\n",
    "        logits = model(torch.tensor([context]))\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        # sample from the distribution\n",
    "        ix = torch.multinomial(probs, num_samples=1).item()\n",
    "        # shift the context window and track the samples\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        # if we sample the special '.' token, break\n",
    "        if ix == 0:\n",
    "            break\n",
    "    \n",
    "    print(''.join(itos[ix] for ix in out)) # decode and print the generated word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52700fee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
